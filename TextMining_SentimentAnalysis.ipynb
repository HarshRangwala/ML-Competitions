{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk \n",
    "import textmining\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory\n",
    "os.chdir(\"D:\\Datasets\\Machine Learning\\Text Mining\\Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Text data\n",
    "post = pd.read_csv(\"ImdbSA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select few text\n",
    "post = post.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Repository\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stop words\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove punctuation marks\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre processing\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \" \".join([ch for ch in stop_free.lower().split() if ch not in exclude])\n",
    "    num_free = \" \".join(i for i in punc_free if not i.isdigit())\n",
    "    return num_free\n",
    "\n",
    "post_corpus = [clean(post.iloc[i,1]) for i in range(0, post.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document term matrix\n",
    "tdm = textmining.TermDocumentMatrix()\n",
    "\n",
    "for i in post_corpus:\n",
    "    \n",
    "    tdm.add_doc(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d0ded5b96b95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Write tdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TDM_DataFRame.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textmining\\__init__.py\u001b[0m in \u001b[0;36mwrite_csv\u001b[1;34m(self, filename, cutoff)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "# Write tdm\n",
    "tdm.write_csv(\"TDM_DataFRame.csv\", cutoff = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe for analysis\n",
    "df = pd.read_csv(\"TDM_DataFRame.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot wordcloud\n",
    "wordcloud = WordCloud(width = 1000, hieght = 500, stopwords = STOPWORDS, background_color = 'white').generate(\n",
    "                        ''.join(post['Post']))\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sentiment analysis using Text Blob\n",
    "# Create empty dataframe to store results\n",
    "FinalResults = pd.DataFrame()\n",
    "\n",
    "# Run Engine\n",
    "for i in range(0, post.shape[0]):\n",
    "    \n",
    "    blob = TextBlob(post.iloc[i,1])\n",
    "    \n",
    "    temp = pd.DataFrame({'Comments': post.iloc[i,1], 'Polarity': blob.sentiment.polarity}, index = [0])\n",
    "    \n",
    "    FinalResults = FinalResults.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis using Vader\n",
    "FinalResults_Vader = pd.DataFrame()\n",
    "\n",
    "# Create engine\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Run Engine\n",
    "for i in range(0, post.shape[0]):\n",
    "    \n",
    "    snt = analyzer.polarity_scores(post.iloc[i,1])\n",
    "    \n",
    "    temp = pd.DataFrame({'Comments': post.iloc[i,1], 'Polarity': list(snt.items())[3][1]}, index = [0])\n",
    "\n",
    "    FinalResults_Vader = FinalResults_Vader.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
